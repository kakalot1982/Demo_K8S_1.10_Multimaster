Instruction for Workshop 2.5 Kubernetes in RealWorld:
Note: This instruction will start lab for kubernetes for real workshop:
====================================================
Lab Description:
VMName:							Machine name		Roles:						IP Address:
kubernetes-ms1					kubernetes-ms1		Master						192.168.99.200
kubernetes-ms2					kubernetes-ms2		Master						192.168.99.201
kubernetes-ms3					kubernetes-ms3		Master						192.168.99.202
#kubernetes-etcd1				kubernetes-etcd1	etcd						192.168.99.203
#kubernetes-etcd2				kubernetes-etcd2	etcd						192.168.99.204
#kubernetes-etcd3				kubernetes-etcd3	etcd						192.168.99.205
kubernetes-wrk1					kubernetes-wrk1		NodePort					192.168.99.206
kubernetes-wrk2					kubernetes-wrk2		NodePort					192.168.99.207
kubernetes-wrk3					kubernetes-wrk3		NodePort					192.168.99.208
===================================================

0. Check region and zone current on GCloud (Note: your zone prefer, project code):
gcloud compute regions list
gcloud compute zones list
gcloud projects list

1. Setup project name "KubernetesProject" and set region/zone to gcloud client:
gcloud config set project kubernetesproject-170714
gcloud config set compute/region asia-east1
gcloud config set compute/zone asia-east1-a

2. Create Network/Firewall for KubernetesLab Group:
./initialnetwork.sh

3. Create Machine for KubernetesLab:
./initialcluster_auto.sh

4. Check state of internet/external ip address (Note for all ip related):
gcloud compute instances list
------------------------------------
Sample Output
------------------------------------
NAME           ZONE          MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP     EXTERNAL_IP     STATUS
kubernetes-ms1   asia-east1-a  n1-standard-1               192.168.99.200  35.234.22.153   RUNNING
kubernetes-ms2   asia-east1-a  n1-standard-1               192.168.99.201  35.189.187.47   RUNNING
kubernetes-ms3   asia-east1-a  n1-standard-1               192.168.99.202  35.234.4.198    RUNNING
kubernetes-wrk1  asia-east1-a  n1-standard-1               192.168.99.206  35.194.180.146  RUNNING
kubernetes-wrk2  asia-east1-a  n1-standard-1               192.168.99.207  35.189.180.87   RUNNING
kubernetes-wrk3  asia-east1-a  n1-standard-1               192.168.99.208  104.199.187.60  RUNNING
------------------------------------

5. Grant permission for docker operate by command:

gcloud compute ssh "kubernetes-ms1"
sudo groupadd docker
sudo usermod -aG docker $USER
sudo systemctl enable docker
exit

gcloud compute ssh "kubernetes-ms2"
sudo groupadd docker
sudo usermod -aG docker $USER
sudo systemctl enable docker
exit

gcloud compute ssh "kubernetes-ms3"
sudo groupadd docker
sudo usermod -aG docker $USER
sudo systemctl enable docker
exit

gcloud compute ssh "kubernetes-wrk1"
sudo groupadd docker
sudo usermod -aG docker $USER
sudo systemctl enable docker
exit

gcloud compute ssh "kubernetes-wrk2"
sudo groupadd docker
sudo usermod -aG docker $USER
sudo systemctl enable docker
exit

gcloud compute ssh "kubernetes-wrk3"
sudo groupadd docker
sudo usermod -aG docker $USER
sudo systemctl enable docker
exit


6. Open ssh (Windows by puttty) with 3 session:
gcloud compute ssh "kubernetes-ms1"
gcloud compute ssh "kubernetes-ms2"
gcloud compute ssh "kubernetes-ms3"
gcloud compute ssh "kubernetes-wrk1"
gcloud compute ssh "kubernetes-wrk2"
gcloud compute ssh "kubernetes-wrk3"

7. For each node initial installation by command: (Skip from Auto)
sudo iptables-save > /tmp/iptables.conf
sudo apt-get update
chmod +x ./initialsetup.sh
./initialsetup.sh
sudo iptables-restore < /tmp/iptables.conf

******Logoff/Logon all node again*****
gcloud compute ssh "kubernetes-ms"
gcloud compute ssh "kubernetes-1"
gcloud compute ssh "kubernetes-2"
**************************************

	
8. (kubernetes-ms1) initial cluster by command:
	sudo su -
	swapoff -a (Also need check and mark on /etc/fstab)
	kubeadm init --pod-network-cidr=192.168.0.0/16 --token 8c2350.f55343444a6ffc46 --apiserver-cert-extra-sans <public ip> (Calico)
	kubeadm init --pod-network-cidr=10.244.0.0/16  --token 8c2350.f55343444a6ffc46 --apiserver-cert-extra-sans <public ip> (Flennel)
	exit

	*Remark: output of this command will generate token that need to keep:
	
9. (kubernetes-ms) Setup run cluster system by command (Regular User):
		  mkdir -p $HOME/.kube
		  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
		  sudo chown $(id -u):$(id -g) $HOME/.kube/config
		  kubectl taint nodes --all node-role.kubernetes.io/master-

10. (local) SCP Certificate from Google Cloud to Local:
	gcloud compute scp kubernetes-ms1:/home/<home directory>/.kube/config adminconfig.conf

11. (kubernetes-ms1) Create calico net plugin for network for cluster by command:
	kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
	kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml

	*Option Flennel
	kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
	kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.8.0/Documentation/kube-flannel-rbac.yml


12. (kubernetes-ms1) Check master readyness and dns by command (Take 5 - 10 min):
	kubectl get pods --all-namespaces
	kubectl describe pods <kube-dns name> --namespace kube-system

	-------------------------------------------------
	Sample Output
	-------------------------------------------------
NAMESPACE     NAME                                     READY     STATUS    RESTARTS   AGE
kube-system   calico-node-qlt5m                        2/2       Running   0          48s
kube-system   etcd-kubernetes-ms1                      1/1       Running   0          16m
kube-system   kube-apiserver-kubernetes-ms1            1/1       Running   0          16m
kube-system   kube-controller-manager-kubernetes-ms1   1/1       Running   0          16m
kube-system   kube-dns-86f4d74b45-fnk6t                0/3       Pending   0          17m
kube-system   kube-proxy-dcb8d                         1/1       Running   0          17m
kube-system   kube-scheduler-kubernetes-ms1            1/1       Running   0          16m
	-------------------------------------------------
13. (local) Edit file adminconfig.conf for change ip address from private to public ip:
vi adminconfig.conf

14. (local) Configure local kubectl for access and command cluster:
kubectl --kubeconfig ./adminconfig.conf get nodes
kubectl --kubeconfig ./adminconfig.conf get svc


Optional:
	1. copy configuration on adminconfig.conf ==> ~/.kube/config (3 Part: cluster, context, users)
	2. kubectl config use-context <context name>
	

15. (kubenetes-ms) Install dashboard by command:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
kubectl apply -f https://github.com/kubernetes/heapster/raw/master/deploy/kube-config/rbac/heapster-rbac.yaml
kubectl apply -f https://github.com/kubernetes/heapster/raw/master/deploy/kube-config/influxdb/heapster.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml
kubectl create clusterrolebinding insecure-dashboard --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard
kubectl get pods --all-namespaces

16. (local) Open dashboard by command:
kubectl --kubeconfig ./adminconfig.conf proxy --accept-hosts '.*'

17. (local) Open browser by command
http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/

18. (kubernetes-wrk1),(kubernetes-wrk2) ssh and join to cluster by command:
	sudo su -
	swapoff -a
	kubeadm join 192.168.99.200:6443 --token 8c2350.f55343444a6ffc46 --discovery-token-ca-cert-hash <sha256 key>
	exit
	-------------------------------------------------
	Sample Output
	-------------------------------------------------
root@kubernetes-1:~# kubeadm join --token 8c2350.f55343444a6ffc46 192.168.99.200:6443 --discovery-token-ca-cert-hash sha256:7c1f1a2c0f578f06b5e449631d38eaeacd47f346cce2e965799d164719dbbf89
[preflight] Running pre-flight checks.
	[WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 17.12.0-ce. Max validated version: 17.03
	[WARNING FileExisting-crictl]: crictl not found in system path
[discovery] Trying to connect to API Server "192.168.99.200:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://192.168.99.200:6443"
[discovery] Requesting info from "https://192.168.99.200:6443" again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "192.168.99.200:6443"
[discovery] Successfully established connection with API Server "192.168.99.200:6443"

This node has joined the cluster:
* Certificate signing request was sent to master and a response
  was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the master to see this node join the cluster.
	-------------------------------------------------

19. (kubernetes-ms1) Check Node in Cluster by command (This take 5 - 10 min):
kubectl get nodes

-------------------------------------------------
Sample Output
-------------------------------------------------
NAME            STATUS    ROLES     AGE       VERSION
kubernetes-1    Ready     <none>    4m        v1.9.2
kubernetes-2    Ready     <none>    23s       v1.9.2
kubernetes-ms   Ready     master    36m       v1.9.2
-------------------------------------------------

20. (kubernetes-ms)Check Pods from all cluster system running by command:
kubectl get pods --all-namespaces

-------------------------------------------------
Sample Output
-------------------------------------------------
NAMESPACE     NAME                                   READY     STATUS    RESTARTS   AGE
kube-system   etcd-kubeserve-ms                      1/1       Running   0          10m
kube-system   kube-apiserver-kubeserve-ms            1/1       Running   0          10m
kube-system   kube-controller-manager-kubeserve-ms   1/1       Running   0          11m
kube-system   kube-dns-2425271678-xhhk6              3/3       Running   0          11m
kube-system   kube-flannel-ds-1h8cv                  2/2       Running   2          7m
kube-system   kube-flannel-ds-83xdr                  2/2       Running   0          6m
kube-system   kube-flannel-ds-n9ws7                  2/2       Running   0          11m
kube-system   kube-proxy-6d7g1                       1/1       Running   0          7m
kube-system   kube-proxy-qzfdr                       1/1       Running   0          6m
kube-system   kube-proxy-vzswv                       1/1       Running   0          11m
kube-system   kube-scheduler-kubeserve-ms            1/1       Running   0          11m
-------------------------------------------------

21. (kubernetes-ms1) Test deployment basic nginx pods by command
kubectl run webtest --image=labdocker/nginx:latest --port=80
kubectl get pods -o wide
kubectl expose deployment webtest --target-port=80 --type=NodePort
kubectl get svc -o wide

22. (kubernetes-ms1) Test get web inside farm by curl:
curl http://192.168.99.200:<port>
curl http://192.168.99.201:<port>
curl http://192.168.99.202:<port>

23. (Client Machine) Test get web outside farm by curl or browser:
gcloud compute instances list
------------------------------------
Sample Output
------------------------------------
NAME           ZONE          MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP     EXTERNAL_IP      STATUS
kubernetes-1   asia-east1-a  n1-standard-1               192.168.99.201  35.194.199.107  RUNNING
kubernetes-2   asia-east1-a  n1-standard-1               192.168.99.202  35.201.184.247  RUNNING
kubernetes-ms  asia-east1-a  n1-standard-1               192.168.99.200  35.201.165.23   RUNNING
------------------------------------

curl http://<External IP of MS>:<port>
curl http://<External IP of MS>:<port>
curl http://<External IP of MS>:<port>

24. (kubeserve-ms1) Cleanup Lab by command:
kubectl delete deployment/webtest
kubectl delete svc/webtest

###################################################################################################
In case of remove node
###################################################################################################
kubectl drain <node name>
kubectl delete node <node name>